---
title: "Investigating Ocean adapt issues"
output:
  html_document: default
  html_notebook: default
---

2018-08-08 - Currently there are 2 issues with Ocean Adapt. The first is that the data structure is not correct for the website - what is stored as a csv, what is stored as a zip file, are all messed up.

Second issue is that there are some lines that are screwing up the SEUS data - Some corruption at the bottom of the SEUS_*.csv files; seems like metadata. This might be interfering with this region being generated.
Cleaning these lines up should allow the script to run properly or at least let us troubleshoot the script further. Last year's SEUS data seems to work properly with the current script.

SEUS_HAUL.csv
Lines: 17208 to 17225
SEUS_CATCH.csv
Lines: 391317 to 391334

**Installing trawldata package to see how it works and if it will help **
This takes forever.  First go round didn't work.
```{r}
# devtools::install_github("rBatt/trawlData") - this doesn't work
# library(trawlData) - won't install the library on whitmore
```
While I'm waiting for trawlData to load, going to explore the output of the OceanAdapt code.  

In 2017, which the website team says worked, in the data_updates directory, I downloaded the zip file called Data_Updated_2017-06-19_11-30-29-EDT.zip. Inside the folder Data_Updated, there are csv's of all of the data.

Here is how it looks:
```{r}
library(dplyr)
file_list <- list.files("~/Downloads/Data_Updated/")

for (i in seq(file_list)){
  x <- readr::read_csv(paste("~/Downloads/Data_Updated/", file_list[i], sep = ""))
  print(file_list[i])
  print(str(x))
}
```
## AI data
```{r}
# ai_data <- readr::read_csv("~/Downloads/Data_Updated/ai_data.csv")

# currently there are 3 different data files on the webpage
# 2014-2016
temp <- tempfile()
download.file("https://www.afsc.noaa.gov/RACE/groundfish/survey_data/downloads/ai2014_2016.zip",temp)
ai_data2014_2016 <- readr::read_csv(unzip(temp))
unlink(temp)

#2002-2012
temp <- tempfile()
download.file("https://www.afsc.noaa.gov/RACE/groundfish/survey_data/downloads/ai2002_2012.zip",temp)
ai_data2002_2012 <- readr::read_csv(unzip(temp))
unlink(temp)

# 1983_2000
temp <- tempfile()
download.file("https://www.afsc.noaa.gov/RACE/groundfish/survey_data/downloads/ai2002_2012.zip",temp)
ai_data1983_2000 <- readr::read_csv(unzip(temp))
unlink(temp)

# join the 3 data files together
ai_data <- rbind(ai_data2014_2016, ai_data2002_2012, ai_data1983_2000)

# are the number of columns the same?
ncol(ai_data) == 17

# are column names the same?
# ai_cols <- names(ai_data)
# save(ai_cols, file = "/Users/Whitmore/Library/Mobile Documents/com~apple~CloudDocs/Documents/OceanAdapt/hist/ai_cols.RDS")
load(file = "/Users/Whitmore/Library/Mobile Documents/com~apple~CloudDocs/Documents/OceanAdapt/hist/ai_cols.RDS")
names(ai_data) == ai_cols

# remove any header rows that have been read in as data
ai_data <- ai_data %>% 
  filter(LATITUDE != "LATITUDE")

```

Switching over to the ms_testing branch of the ocean adapt code to see if I can wrangle this all out into something that makes sense.
