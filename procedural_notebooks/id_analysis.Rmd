---
title: "Identity Analysis of APCL"
output:
  pdf_document: default
  html_notebook: default
<<<<<<< HEAD
||||||| merged common ancestors
date: '2018-06-07'
=======
date: "`r Sys.Date()"
>>>>>>> 28e5d2d65d2156aa3c9240d8800a55a0da098442
---

This cervus identity analysis was conducted by KAC on the individuals included in APCL sequencing runs 03-17 (majority of individuals captured in 2012-2015, <50 from 2016).

1. Add "pop" line to genepop in text editor. 
*File "SNP.DP3g95p5maf05HM_seq_17_03.LDpruned.beta.noissues.IDfin.gen" is in the id_support folder.*

2. In cervus, **convert genepop** to cervus file using the "Tools" menu. 
This takes seconds. 
*File "SNP.DP3g95p5maf05HM_seq_17_03.LDpruned.beta.noissues.IDfin.csv" is in the id_support folder*
  - note the number of loci - **`r (num_loci <- 2253)`**

3. Using the "Analysis" menu, conduct an **allele frequency analysis**.  
This takes seconds.  
*File "SNP.DP3g95p5maf05HM_seq_17_03.LDpruned.beta.noissues.IDfin_AF.alf" is in the id_support folder.*
  i. use the file just created by converting the genepop.
  ii. make sure "header row"" and "read locus names"" are checked.
  iii. ID is in column 2
  iv. first allele is in column 3
  v. fill in the number of loci listed in the conversion step
  vi. save as input_file_name_AF
  vii. Do not do Hardy Weinberg
  viii. Do not estimate null allele frequency
  
4. Using the "Analysis" menu, conduct **identity analysis**.
MRS notes from the past said this took 30 seconds with 809 loci, KAC notes from 2253 loci in this analysis took 1 minute, 20 seconds.
*Files "SNP.DP3g95p5maf05HM_seq_17_03.LDpruned.beta.noissues.IDfin.ident.txt" and "SNP.DP3g95p5maf05HM_seq_17_03.LDpruned.beta.noissues.IDfin.ident.csv" are in the id_support folder.*
  i.Genotype file and allele frequency info will be automatically populated, should match what youâ€™ve done for those steps.
  ii. Header should be checked
  iii. ID in column 2
  iv. First allele in column 3
  v. Do not test sexes separately
  vi. Save summary output file using the same naming scheme
  vii. Minimum number of matching loci should be 80% of the total number of loci - **`r  (min_match <- floor(0.8 * num_loci))`**
  viii. Allow fuzzy matching with **`r (mismatch <- floor(0.1 * num_loci))`**
  ix. Do not show all comparisons
  
*MRS to run the remaining identity analysis in R*
<<<<<<< HEAD

1. Setup workspace, load and format data
```{r load_functions, echo=FALSE, message=FALSE}
||||||| merged common ancestors
```{r echo=FALSE, message=FALSE}
=======

1. Setup workspace, load and format data
```{r echo=FALSE, message=FALSE}
>>>>>>> 28e5d2d65d2156aa3c9240d8800a55a0da098442
source("../genomics/scripts/lab_helpers.R")
source("../genomics/scripts/gen_helpers.R")
library(kableExtra)

```
```{r load_format_data}

# infile <- "id_support/SNP.DP3g95p5maf05HM.HWE.recode.LDpruned.ID2.noissues_ID.csv"

# KAC created a file of samples that are of special interest and not regenotypes
infile <- "id_support/notregenos_notin809ID.csv"

# Import cervus identity results ------------------------------------------
idcsv <- readr::read_csv(infile)
# filtered <- readr::read_csv(filtered)

# convert column names to lower case
names(idcsv) <- stringr::str_to_lower(names(idcsv))
<<<<<<< HEAD
# convert spaces in names to underscores
names(idcsv) <- stringr::str_replace(names(idcsv), " ", "_")
||||||| merged common ancestors
=======
# convert spaces in names to underscores
names(idcsv) <- stringr::str_replace(names(idcsv), " ", "_")

# if necessary, strip ids down to ligation id only
idcsv <- idcsv %>% 
  mutate(first_id = ifelse(nchar(first_id) == 15, paste("APCL_", substr(first_id, 11, 15), sep = ""), ifelse(nchar(first_id) == 10, substr(first_id, 6, 10), first_id)), 
    second_id = ifelse(nchar(second_id) == 15, paste("APCL_", substr(second_id, 11, 15), sep = ""), ifelse(nchar(second_id) == 10, substr(second_id, 6, 10), second_id)))
```  
2. Add metadata  
```{r metadata}
# Connect to database -----------------------------------------------------
lab <- read_db("Laboratory")

# add Sample IDs
lab1 <- idcsv %>% 
  rename(ligation_id = first_id)
lab1 <- samp_from_lig(lab1)

lab2 <- idcsv %>% 
  rename(ligation_id = second_id)
lab2 <- samp_from_lig(lab2)

# change names to represent first fish
names(lab1) <- paste("first_", names(lab1), sep = "")
names(lab2) <- paste("second_", names(lab2), sep = "")

# add to full table
idcsv <- left_join(idcsv, lab1, by = c("first_id" = "first_ligation_id"))
idcsv <- left_join(idcsv, lab2, by = c("second_id" = "second_ligation_id"))
```
3. Check the proportion of mismatches.  In the past there was "a trail of smoke" that spread down to low numbers of loci and high mismatch proportion.  Make sure there is no "trail of smoke"
```{r mismatches}
# check proportion of matches/mismatches
idcsv <- idcsv %>% 
  mutate(mismatch_prop = mismatching_loci/(mismatching_loci + matching_loci))

plot(mismatch_prop ~ matching_loci, idcsv, bty = "n", las = 1, xlim = c(600,1100), ylim = c(0,0.15))

# bty "l" is this type, "n" is no box
#las 1 is all labels horizontal, 2 is always perpendicular to axis, 3 is always vertical.

# clean up
rm(lab1, lab2)
```
4. Add field data
```{r field_data}
leyte <- read_db("Leyte")

samp1 <- idcsv %>% 
  select(first_sample_id)

samp1 <- full_meta(samp1$first_sample_id, leyte) %>% 
  select(-fish_table_id, -contains("notes"), -contains("collector"), -divers, -gps, -dive_num)

samp2 <- idcsv %>% 
  select(second_sample_id)

samp2 <- full_meta(samp2$second_sample_id, leyte) %>% 
  select(-fish_table_id, -contains("notes"), -contains("collector"), -divers, -gps, -dive_num)

names(samp1) <- paste("first", names(samp1), sep = "_")
names(samp2) <- paste("second", names(samp2), sep = "_")

idcsv <- left_join(idcsv, samp1, by = "first_sample_id")
idcsv <- left_join(idcsv, samp2, by = "second_sample_id")

rm(samp1, samp2)
```

Add lat long data to first set of individuals
```{r}
library(lubridate)

# convert obs_times to UTC
gpx1 <- idcsv %>% 
  select(first_sample_id, first_anem_obs_time, first_date, first_anem_id, first_dive_table_id) %>% 
  mutate(obs_time = force_tz(ymd_hms(str_c(first_date, first_anem_obs_time, sep = " ")), tzone = "Asia/Manila"),
    obs_time = with_tz(obs_time, tzone = "UTC"), 
    gpx_date = date(obs_time),
    hour = hour(obs_time),
    minute = minute(obs_time),  
    sec = second(obs_time))

dive1 <- leyte %>%
    tbl("diveinfo") %>%
    filter(dive_table_id %in% gpx1$first_dive_table_id) %>%
    select(dive_table_id, gps) %>%
    collect()
gpx1 <- left_join(gpx1, dive1, by = c("first_dive_table_id"  = "dive_table_id")) %>% 
  rename(unit = gps, 
    gpx_hour = hour) %>% 
  mutate(gpx_date = as.character(gpx_date))
rm(dive1)

lat1 <- leyte %>%
  tbl("GPX")  %>% 
  mutate(gpx_date = date(time)) %>%
  filter(gpx_date %in% gpx1$gpx_date) %>% 
  mutate(gpx_hour = hour(time), 
    minute = minute(time), 
    second = second(time)) %>%
  select(-time, -second)%>%
  collect() 

sum_lat1 <- lat1 %>%
  group_by(unit, gpx_date, gpx_hour, minute) %>% 
  summarise(lat = mean(as.numeric(lat)),
    lon = mean(as.numeric(lon)))

gpx1 <- left_join(gpx1, sum_lat1, by = c("unit", "gpx_date", "gpx_hour", "minute"))

gpx1 <- gpx1 %>% 
  select(first_sample_id, lat, lon) %>% 
  rename(first_lat = lat, 
    first_lon = lon)
```

Add lat long to the second set of individuals
```{r}
library(lubridate)

# convert obs_times to UTC
gpx2 <- idcsv %>% 
  select(second_sample_id, second_anem_obs_time, second_date, second_anem_id, second_dive_table_id) %>% 
  mutate(obs_time = force_tz(ymd_hms(str_c(second_date, second_anem_obs_time, sep = " ")), tzone = "Asia/Manila"),
    obs_time = with_tz(obs_time, tzone = "UTC"), 
    gpx_date = date(obs_time),
    hour = hour(obs_time),
    minute = minute(obs_time),  
    sec = second(obs_time))

dive2 <- leyte %>%
    tbl("diveinfo") %>%
    filter(dive_table_id %in% gpx2$second_dive_table_id) %>%
    select(dive_table_id, gps) %>%
    collect()
gpx2 <- left_join(gpx2, dive2, by = c("second_dive_table_id"  = "dive_table_id")) %>% 
  rename(unit = gps, 
    gpx_hour = hour) %>% 
  mutate(gpx_date = as.character(gpx_date))
rm(dive2)

lat2 <- leyte %>%
  tbl("GPX")  %>% 
  mutate(gpx_date = date(time)) %>%
  filter(gpx_date %in% gpx2$gpx_date) %>% 
  mutate(gpx_hour = hour(time), 
    minute = minute(time), 
    second = second(time)) %>%
  select(-time, -second)%>%
  collect() 

sum_lat2 <- lat2 %>%
  group_by(unit, gpx_date, gpx_hour, minute) %>% 
  summarise(lat = mean(as.numeric(lat)),
    lon = mean(as.numeric(lon)))

gpx2 <- left_join(gpx2, sum_lat2, by = c("unit", "gpx_date", "gpx_hour", "minute")) 

gpx2 <- gpx2 %>% 
  select(second_sample_id, lat, lon) %>% 
  rename(second_lat = lat, 
    second_lon = lon)
```

Add these lat longs back into the main idcsv file
```{r}
idcsv <- left_join(idcsv, gpx1, by = "first_sample_id") %>% 
  distinct()

idcsv <- left_join(idcsv, gpx2, by = "second_sample_id") %>% 
  distinct()

# cleanup
rm(gpx1, gpx2, lat1, lat2, sum_lat1, sum_lat2)
```
** Now that the data has been compiled and the formatting has been fixed, we can begin quality analysis**

1. Look for matches with same date of capture - create a column called "date_eval" and any pair that were captured on the same date will have a "FAIL" value in that column
```{r}
idcsv <- idcsv %>% 
  mutate(date_eval = ifelse(first_date == second_date, "FAIL", NA))
```

# Flag matches that were caught more than 250m apart


----------------------
# library(fields)
# source('greatcircle_funcs.R') # alternative, probably faster

```{r}
alldists <- fields::rdist.earth(as.matrix(idcsv[,c('first_lon', 'first_lat')]), as.matrix(idcsv[,c('second_lon', 'second_lat')]), miles=FALSE, R=6371) # see http://www.r-bloggers.com/great-circle-distance-calculations-in-r/ # slow because it does ALL pairwise distances, instead of just in order
idcsv$distkm <- diag(alldists)

idcsv <- idcsv %>% 
  mutate(dist_eval = ifelse(distkm >= 0.250, "FAIL", NA))

dist_fails <- idcsv %>% 
  filter(dist_eval == "FAIL") %>% 
  select(first_site, second_site, everything())

```

# Flag idcsves where size decreases by more than 1.5cm --------------------
```{r}
idcsv <- idcsv %>% 
  mutate(size_eval = ifelse(first_date < second_date & (as.numeric(first_size)-1.5) > as.numeric(second_size), "FAIL", NA))

idcsv <- idcsv %>% 
  mutate(size_eval = ifelse(first_date > second_date & as.numeric(first_size) < (as.numeric(second_size)-1.5), "FAIL", size_eval))
>>>>>>> 28e5d2d65d2156aa3c9240d8800a55a0da098442

<<<<<<< HEAD
# if necessary, strip ids down to ligation id only
||||||| merged common ancestors
idcsv <- slice(idcsv, 1:10)

# if necessary, strip IDs down to ligation id only
=======
size_fails <- idcsv %>% 
  filter(size_eval == "FAIL") %>% 
  select(first_sample_id, first_size, second_sample_id, second_size, first_date, second_date, everything())
```

Check for site differences
```{r}
>>>>>>> 28e5d2d65d2156aa3c9240d8800a55a0da098442
idcsv <- idcsv %>% 
<<<<<<< HEAD
  mutate(first_id = ifelse(nchar(first_id) == 15, paste("APCL_", substr(first_id, 11, 15), sep = ""), ifelse(nchar(first_id) == 10, substr(first_id, 6, 10), first_id)), 
    second_id = ifelse(nchar(second_id) == 15, paste("APCL_", substr(second_id, 11, 15), sep = ""), ifelse(nchar(second_id) == 10, substr(second_id, 6, 10), second_id)))
```  
2. Add metadata  
```{r metadata}
||||||| merged common ancestors
  mutate(first.id = ifelse(nchar(first.id) == 15, paste("APCL_", substr(first.id, 11, 15), sep = ""), ifelse(nchar(first.id) == 10, substr(first.id, 6, 10), first.id)), 
    second.id = ifelse(nchar(second.id) == 15, paste("APCL_", substr(second.id, 11, 15), sep = ""), ifelse(nchar(second.id) == 10, substr(second.id, 6, 10), second.id)))
  

# Add metadata ------------------------------------------------------------

=======
  mutate(site_eval = ifelse(first_site != second_site, "FAIL", NA))

site_fails <- idcsv %>% 
  filter(site_eval == "FAIL")
select(first_sample_id, second_sample_id )
```






# Write output ------------------------------------------------------------

write.csv(idcsv, file = paste("data/", Sys.Date(), "_idanalyis.csv", sep = ""), row.names = F)

# cleanup
# rm(alldists, c5, first, lab1, lab2, latlong, second, a, b, c1, c2, c3, c4, date, datesplit, day, hour, i, i2, latlongindex, min, month, sec, time, timesplit, year)

### EVERYTHING AFTER THIS POINT IS FOR REMOVING THE MATCHES FROM THE GENEPOP
# SO IT CAN CONTINUE FOR PARENTAGE ANALYSIS.  FOR CONTINUED ID ANALYSIS, OPEN id_process.R ###

# Open genepop ------------------------------------------------------------

genfile <- "data/2016-12-20_noregeno.gen" # this should be the genepop you used as input for Cervus ID
genedf <- readGenepop(genfile)

### WAIT ###

genedf$pop <- NULL # remove the pop column from the data file
# TEST - make sure the first 2 columns are names and a contig and get number of rows
names(genedf[,1:2]) # [1] "names" "dDocent_Contig_107_30"
nrow(genedf) # 1824


# Calculate the number of loci for analysis -------------------------------

# convert 0000 to NA in the genepop data
genedf[genedf == "0000"] = NA
# TEST - make sure there are no "0000" left
which(genedf == "0000") # should return integer(0)

# count the number of loci per individual
for(h in 1:nrow(genedf)){
  genedf$numloci[h] <- sum(!is.na(genedf[h,]))
}

### WAIT ###

# TEST - make sure all of the numloci were populated
which(is.na(genedf$numloci)) # should return integer(0)

genedf$drop <- NA

# Run through id analysis and compare to determine which to remove --------
for(i in 1:nrow(idcsv)){
  # a & b are  the line numbers from genepop file that matches an the first and second ID in the match table
  a <- which(genedf$names == idcsv$First_id[i])
  b <- which(genedf$names == idcsv$Second_id[i])
if (genedf$numloci[a] > genedf$numloci[b]){
  genedf$drop[b] <- "DROP"
} else{
  genedf$drop[a] <- "DROP"
}
}

# Make a dataframe of the samples that will be dropped
drops <- genedf[!is.na(genedf$drop),]

# Make a dataframe of the samples to keep for parentage analysis
keep <- genedf[is.na(genedf$drop),]

keep$numloci <- NULL
keep$drop <- NULL

# TODO -  Look for regenotypes again:

# convert all the NA genotypes to 0000
keep[is.na(keep)] = "0000"
# TEST - make sure there are no NA's left
which(is.na(keep)) # should return integer(0)

# Write out genepop  ------------------------------------------------------

# Build the genepop components
msg <- c("This genepop file was generated using a script called identity_analysis.R written by Michelle Stuart with help from Malin Pinsky and Ryan Batt")

loci <- paste(names(keep[,2:ncol(keep)]), collapse =",")

gene <- vector()
sample <- vector()
for (i in 1:nrow(keep)){
  gene[i] <- paste(keep[i,2:ncol(keep)], collapse = " ")
  sample[i] <- paste(keep[i,1], gene[i], sep = ", ")
}

  ### WAIT ###

out <- c(msg, loci, 'pop', sample)

write.table(out, file = paste("data/",Sys.Date(), "_norecap.gen", sep = ""), row.names=FALSE, quote=FALSE, col.names=FALSE)



```


---
title: "Identity Analysis"
output: html_notebook
---
This script evaluates the output of a cervus identity analysis and flags "true matches" versus "false positives"
**TODO** - make a list of good matches and a list of bad matches and see where they intersect, then drop bad part of the bad match sample but not the good part. - how to define good match - 


1. Set up your workspace
```{r workspace}
# load useful functions
source("../genomics/scripts/gen_helpers.R")

# load cervus ID analysis output
idcsv <- readr::read_csv("procedural_notebooks/id_support/SNP.DP3g95p5maf05HM_seq_17_03.LDpruned.beta.noissues.IDfin.csv")
```


# Import cervus identity results ------------------------------------------
  

# # # if necessary, strip IDs down to ligation id only
# for (i in 1:nrow(idcsv)){
#   if(nchar(idcsv$First.ID[i]) == 15){
#     idcsv$First.ID[i] <- paste("APCL_", substr(idcsv$First.ID[i], 11, 15), sep = "")
#   }
#   if(nchar(idcsv$First.ID[i]) == 10){
#     idcsv$First.ID[i] <- substr(idcsv$First.ID[i], 6, 10)
#   }
# }
# for (i in 1:nrow(idcsv)){
#   if(nchar(idcsv$Second.ID[i]) == 15){
#     idcsv$Second.ID[i] <- paste("APCL_", substr(idcsv$Second.ID[i], 11, 15), sep = "")
#   }
#   if(nchar(idcsv$Second.ID[i]) == 10){
#     idcsv$Second.ID[i] <- substr(idcsv$Second.ID[i], 6, 10)
#   }
# }

# Add metadata ------------------------------------------------------------

>>>>>>> 28e5d2d65d2156aa3c9240d8800a55a0da098442
# Connect to database -----------------------------------------------------
lab <- read_db("Laboratory")

# add Sample IDs
lab1 <- idcsv %>% 
  rename(ligation_id = first_id)
lab1 <- samp_from_lig(lab1)

lab2 <- idcsv %>% 
  rename(ligation_id = second_id)
lab2 <- samp_from_lig(lab2)

# change names to represent first fish
names(lab1) <- paste("first_", names(lab1), sep = "")
names(lab2) <- paste("second_", names(lab2), sep = "")

# add to full table
idcsv <- left_join(idcsv, lab1, by = c("first_id" = "first_ligation_id"))
idcsv <- left_join(idcsv, lab2, by = c("second_id" = "second_ligation_id"))
```
3. Check the proportion of mismatches.  In the past there was "a trail of smoke" that spread down to low numbers of loci and high mismatch proportion.  Make sure there is no "trail of smoke"
```{r mismatches}
# check proportion of matches/mismatches
idcsv <- idcsv %>% 
  mutate(mismatch_prop = mismatching_loci/(mismatching_loci + matching_loci))

plot(mismatch_prop ~ matching_loci, idcsv, bty = "n", las = 1, xlim = c(600,1100), ylim = c(0,0.15))

# bty "l" is this type, "n" is no box
#las 1 is all labels horizontal, 2 is always perpendicular to axis, 3 is always vertical.

# clean up
rm(lab1, lab2)
```
4. Add field data
```{r field_data}
leyte <- read_db("Leyte")

samp1 <- idcsv %>% 
  select(first_sample_id)

samp1 <- full_meta(samp1$first_sample_id, leyte) %>% 
  select(-fish_table_id, -contains("notes"), -contains("collector"), -divers, -gps, -dive_num)

samp2 <- idcsv %>% 
  select(second_sample_id)

samp2 <- full_meta(samp2$second_sample_id, leyte) %>% 
  select(-fish_table_id, -contains("notes"), -contains("collector"), -divers, -gps, -dive_num)

names(samp1) <- paste("first", names(samp1), sep = "_")
names(samp2) <- paste("second", names(samp2), sep = "_")

idcsv <- left_join(idcsv, samp1, by = "first_sample_id")
idcsv <- left_join(idcsv, samp2, by = "second_sample_id")

rm(samp1, samp2)
```

Add lat long data to first set of individuals
```{r}
library(lubridate)

# convert obs_times to UTC
gpx1 <- idcsv %>% 
  select(first_sample_id, first_anem_obs_time, first_date, first_anem_id, first_dive_table_id) %>% 
  mutate(obs_time = force_tz(ymd_hms(str_c(first_date, first_anem_obs_time, sep = " ")), tzone = "Asia/Manila"),
    obs_time = with_tz(obs_time, tzone = "UTC"), 
    gpx_date = date(obs_time),
    hour = hour(obs_time),
    minute = minute(obs_time),  
    sec = second(obs_time))

dive1 <- leyte %>%
    tbl("diveinfo") %>%
    filter(dive_table_id %in% gpx1$first_dive_table_id) %>%
    select(dive_table_id, gps) %>%
    collect()
gpx1 <- left_join(gpx1, dive1, by = c("first_dive_table_id"  = "dive_table_id")) %>% 
  rename(unit = gps, 
    gpx_hour = hour) %>% 
  mutate(gpx_date = as.character(gpx_date))
rm(dive1)

lat1 <- leyte %>%
  tbl("GPX")  %>% 
  mutate(gpx_date = date(time)) %>%
  filter(gpx_date %in% gpx1$gpx_date) %>% 
  mutate(gpx_hour = hour(time), 
    minute = minute(time), 
    second = second(time)) %>%
  select(-time, -second)%>%
  collect() 

sum_lat1 <- lat1 %>%
  group_by(unit, gpx_date, gpx_hour, minute) %>% 
  summarise(lat = mean(as.numeric(lat)),
    lon = mean(as.numeric(lon)))

gpx1 <- left_join(gpx1, sum_lat1, by = c("unit", "gpx_date", "gpx_hour", "minute"))

gpx1 <- gpx1 %>% 
  select(first_sample_id, lat, lon) %>% 
  rename(first_lat = lat, 
    first_lon = lon)
```

Add lat long to the second set of individuals
```{r}
library(lubridate)

# convert obs_times to UTC
gpx2 <- idcsv %>% 
  select(second_sample_id, second_anem_obs_time, second_date, second_anem_id, second_dive_table_id) %>% 
  mutate(obs_time = force_tz(ymd_hms(str_c(second_date, second_anem_obs_time, sep = " ")), tzone = "Asia/Manila"),
    obs_time = with_tz(obs_time, tzone = "UTC"), 
    gpx_date = date(obs_time),
    hour = hour(obs_time),
    minute = minute(obs_time),  
    sec = second(obs_time))

dive2 <- leyte %>%
    tbl("diveinfo") %>%
    filter(dive_table_id %in% gpx2$second_dive_table_id) %>%
    select(dive_table_id, gps) %>%
    collect()
gpx2 <- left_join(gpx2, dive2, by = c("second_dive_table_id"  = "dive_table_id")) %>% 
  rename(unit = gps, 
    gpx_hour = hour) %>% 
  mutate(gpx_date = as.character(gpx_date))
rm(dive2)

lat2 <- leyte %>%
  tbl("GPX")  %>% 
  mutate(gpx_date = date(time)) %>%
  filter(gpx_date %in% gpx2$gpx_date) %>% 
  mutate(gpx_hour = hour(time), 
    minute = minute(time), 
    second = second(time)) %>%
  select(-time, -second)%>%
  collect() 

sum_lat2 <- lat2 %>%
  group_by(unit, gpx_date, gpx_hour, minute) %>% 
  summarise(lat = mean(as.numeric(lat)),
    lon = mean(as.numeric(lon)))

gpx2 <- left_join(gpx2, sum_lat2, by = c("unit", "gpx_date", "gpx_hour", "minute")) 

gpx2 <- gpx2 %>% 
  select(second_sample_id, lat, lon) %>% 
  rename(second_lat = lat, 
    second_lon = lon)
```

Add these lat longs back into the main idcsv file
```{r}
idcsv <- left_join(idcsv, gpx1, by = "first_sample_id") %>% 
  distinct()

idcsv <- left_join(idcsv, gpx2, by = "second_sample_id") %>% 
  distinct()

# cleanup
rm(gpx1, gpx2, lat1, lat2, sum_lat1, sum_lat2)
```
** Now that the data has been compiled and the formatting has been fixed, we can begin quality analysis**

1. Look for matches with same date of capture - create a column called "date_eval" and any pair that were captured on the same date will have a "FAIL" value in that column
```{r}
idcsv <- idcsv %>% 
  mutate(date_eval = ifelse(first_date == second_date, "FAIL", NA))
```

Flag matches that were caught more than 250m apart
```{r}
# library(fields)
# source('greatcircle_funcs.R') # alternative, probably faster
alldists <- fields::rdist.earth(as.matrix(idcsv[,c('first_lon', 'first_lat')]), as.matrix(idcsv[,c('second_lon', 'second_lat')]), miles=FALSE, R=6371) # see http://www.r-bloggers.com/great-circle-distance-calculations-in-r/ # slow because it does ALL pairwise distances, instead of just in order
idcsv$distkm <- diag(alldists)

idcsv <- idcsv %>% 
  mutate(dist_eval = ifelse(distkm >= 0.250, "FAIL", NA))

dist_fails <- idcsv %>% 
  filter(dist_eval == "FAIL") %>% 
  select(first_site, second_site, everything())

kable(dist_fails, booktabs = T) %>% 
  kable_styling(latex_options = "scale_down")

```

Flag individuals where size decreases by more than 1.5cm.
```{r}
idcsv <- idcsv %>% 
  mutate(size_eval = ifelse(first_date < second_date & (as.numeric(first_size)-1.5) > as.numeric(second_size), "FAIL", NA))

idcsv <- idcsv %>% 
  mutate(size_eval = ifelse(first_date > second_date & as.numeric(first_size) < (as.numeric(second_size)-1.5), "FAIL", size_eval))

size_fails <- idcsv %>% 
  filter(size_eval == "FAIL") %>% 
  select(first_sample_id, first_size, second_sample_id, second_size, first_date, second_date, first_id, second_id)
kable(size_fails, booktabs = T) %>% 
  kable_styling(latex_options = "scale_down")

```

Check for site differences
```{r}
idcsv <- idcsv %>% 
  mutate(site_eval = ifelse(first_site != second_site, "FAIL", NA))

site_fails <- idcsv %>% 
  filter(site_eval == "FAIL") %>% 
  select(first_sample_id, second_sample_id, first_site, second_site, first_size, second_size, first_color, second_color, first_id, second_id)
kable(site_fails, booktabs = T) %>% 
  kable_styling(latex_options = "scale_down")
```
Check for samples that were captured in the same year but vary in size by more than 1.5cm
```{r same_year_size}
idcsv <- idcsv %>% 
  mutate(year_size_eval = ifelse(year(first_date) == year(second_date) & first_size != second_size, "FAIL", NA))

year_size_fails <- idcsv %>% 
  filter(year_size_eval == "FAIL") %>% 
  select(first_sample_id, second_sample_id, first_site, second_site, first_size, second_size, first_color, second_color, first_id, second_id, first_date, second_date)

year_size_fails <- year_size_fails %>% 
  filter(first_sample_id == "APCL14_500" | first_sample_id == "APCL12_191")

kable(year_size_fails, booktabs = T) %>% 
  kable_styling(latex_options = "scale_down")
```

2 of these "year_size_fails" are close enough in size to each other to be believeable, but 2 appear to be errors somewhere.


At this point, I take the fails to evernote and check if we've noted these samples in red flag situations in the past.

```{r generate_text_for_evernote, echo=FALSE}
for(i in seq(nrow(site_fails))){
print(paste(site_fails$first_sample_id[i], " was identified as ", site_fails$first_id[i], " captured in ", site_fails$first_site[i], " at ", site_fails$first_size[i], "cm with tail color ", site_fails$first_color[i], ".  It matched genetically to ", site_fails$second_sample_id[i], " identified as ", site_fails$second_id[i], " captured in ", site_fails$second_site[i], " at ", site_fails$second_size[i], "cm with tail color ", site_fails$second_color[i], ". This raised a red flag because the sites don't match", sep = ""))
}

```


At this point I will get the work history for individual fish
```{r bind_year_size_fails}
# create a table of all info for the first fish
firsts <- year_size_fails %>% 
  select(contains("first"))
# rename columns to remove "first_"
names(firsts) <- str_replace(names(firsts), "first_", "")
# create a table of all info for the second fish
seconds <- year_size_fails %>% 
  select(contains("second"))
# rename columns to remove "second_"
names(seconds) <- str_replace(names(seconds), "second_", "")

# bind the 2 sets of fish together
both <- rbind(firsts, seconds) %>% 
  rename(ligation_id = id)
```
Double check the data sheets to make sure there are no type-os - found one type-o but the match is still suspicious because of size differences and site differences within the same year.

Get the work history for these samples
```{r get_work_history}
hist <- work_history(both, "ligation_id")

kable(hist)
```

L0969 is in my notes as a sample that failed sequencing.  Let's grab the sequencing results for these ligations
```{r}
seq <- lab %>%
  tbl("ligation") %>% 
  filter(ligation_id %in% hist$ligation_id) %>% 
  select(ligation_id, total_reads, lack_rad_tag, low_quality, retained, plate, well) %>% 
  collect()

hist <- left_join(hist, seq, by = "ligation_id")

hist <- left_join(hist, select(idcsv, loci_typed, first_id), by = c("ligation_id" = "first_id"))
hist <- left_join(hist, select(idcsv, loci_typed.1, second_id), by = c("ligation_id" = "second_id"))

hist <- hist %>% 
  mutate(loci_typed = ifelse(is.na(loci_typed), loci_typed.1, loci_typed)) %>% 
  select(-loci_typed.1)
```
Get plate maps for the ligations
```{r}
for (i in seq(nrow(hist))){
  plate <- lab %>% 
    tbl("ligation") %>% 
    filter(plate == hist$plate[i]) %>% 
    mutate(trouble = ifelse(ligation_id == hist$ligation_id[i], "YES", NA)) %>%
    mutate(row = substr(well, 1, 1),
      col = substr(well, 2, 3)) %>% 
    select(row, col, contains("id"), trouble) %>%
      collect()
  
  plate <- plate %>% 
    mutate(
      row = factor(row, levels = c("H", "G", "F", "E", "D", "C", "B", "A")), 
      col = factor(col, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)))


 plateheatmap <- ggplot(plate, mapping = aes(x = col, y = row, fill = trouble))+
   scale_fill_brewer(palette = "Blues", na.value = "white")+
    geom_tile(color = "black")
  plateheatmap +
    geom_text(aes(col, row, label = ligation_id), color = "black", size = 4)
  ggsave(paste("plots/", hist$plate[i], ".pdf", sep = ""))
  
}
<<<<<<< HEAD
```
i


||||||| merged common ancestors

  ### WAIT ###

out <- c(msg, loci, 'pop', sample)

write.table(out, file = paste("data/",Sys.Date(), "_norecap.gen", sep = ""), row.names=FALSE, quote=FALSE, col.names=FALSE)



```

=======

  ### WAIT ###

out <- c(msg, loci, 'pop', sample)

write.table(out, file = paste("data/",Sys.Date(), "_norecap.gen", sep = ""), row.names=FALSE, quote=FALSE, col.names=FALSE)



>>>>>>> 28e5d2d65d2156aa3c9240d8800a55a0da098442
